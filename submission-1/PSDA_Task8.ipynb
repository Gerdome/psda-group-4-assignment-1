{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"PSDA_Task8.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"y4Y2Wm_7W4dg","colab_type":"code","outputId":"bafa87a9-7fa9-46e6-f1b9-43df75f2be89","executionInfo":{"status":"ok","timestamp":1589146857584,"user_tz":-120,"elapsed":1389,"user":{"displayName":"Gerrit Merz","photoUrl":"","userId":"17174744278973723577"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","basedir = '/content/drive/My Drive/PSDA Group 4/Data/'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UBQChU8FRt9G","colab_type":"code","colab":{}},"source":["from sklearn import preprocessing\n","import pandas as pd\n","import numpy as np\n","import csv\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_selection import RFECV"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Th57v8RjRt9P","colab_type":"code","colab":{}},"source":["data_path = basedir + 'wdbc.data'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yLbTe9lwRt9X","colab_type":"code","outputId":"f61bd34b-0463-4889-e97e-575f1fd060cb","executionInfo":{"status":"ok","timestamp":1589146863377,"user_tz":-120,"elapsed":1801,"user":{"displayName":"Gerrit Merz","photoUrl":"","userId":"17174744278973723577"}},"colab":{"base_uri":"https://localhost:8080/","height":439}},"source":["data = pd.read_csv(data_path,header=None)\n","data #check the rows and columns of this dataset"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>842302</td>\n","      <td>M</td>\n","      <td>17.99</td>\n","      <td>10.38</td>\n","      <td>122.80</td>\n","      <td>1001.0</td>\n","      <td>0.11840</td>\n","      <td>0.27760</td>\n","      <td>0.30010</td>\n","      <td>0.14710</td>\n","      <td>0.2419</td>\n","      <td>0.07871</td>\n","      <td>1.0950</td>\n","      <td>0.9053</td>\n","      <td>8.589</td>\n","      <td>153.40</td>\n","      <td>0.006399</td>\n","      <td>0.04904</td>\n","      <td>0.05373</td>\n","      <td>0.01587</td>\n","      <td>0.03003</td>\n","      <td>0.006193</td>\n","      <td>25.380</td>\n","      <td>17.33</td>\n","      <td>184.60</td>\n","      <td>2019.0</td>\n","      <td>0.16220</td>\n","      <td>0.66560</td>\n","      <td>0.7119</td>\n","      <td>0.2654</td>\n","      <td>0.4601</td>\n","      <td>0.11890</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>842517</td>\n","      <td>M</td>\n","      <td>20.57</td>\n","      <td>17.77</td>\n","      <td>132.90</td>\n","      <td>1326.0</td>\n","      <td>0.08474</td>\n","      <td>0.07864</td>\n","      <td>0.08690</td>\n","      <td>0.07017</td>\n","      <td>0.1812</td>\n","      <td>0.05667</td>\n","      <td>0.5435</td>\n","      <td>0.7339</td>\n","      <td>3.398</td>\n","      <td>74.08</td>\n","      <td>0.005225</td>\n","      <td>0.01308</td>\n","      <td>0.01860</td>\n","      <td>0.01340</td>\n","      <td>0.01389</td>\n","      <td>0.003532</td>\n","      <td>24.990</td>\n","      <td>23.41</td>\n","      <td>158.80</td>\n","      <td>1956.0</td>\n","      <td>0.12380</td>\n","      <td>0.18660</td>\n","      <td>0.2416</td>\n","      <td>0.1860</td>\n","      <td>0.2750</td>\n","      <td>0.08902</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>84300903</td>\n","      <td>M</td>\n","      <td>19.69</td>\n","      <td>21.25</td>\n","      <td>130.00</td>\n","      <td>1203.0</td>\n","      <td>0.10960</td>\n","      <td>0.15990</td>\n","      <td>0.19740</td>\n","      <td>0.12790</td>\n","      <td>0.2069</td>\n","      <td>0.05999</td>\n","      <td>0.7456</td>\n","      <td>0.7869</td>\n","      <td>4.585</td>\n","      <td>94.03</td>\n","      <td>0.006150</td>\n","      <td>0.04006</td>\n","      <td>0.03832</td>\n","      <td>0.02058</td>\n","      <td>0.02250</td>\n","      <td>0.004571</td>\n","      <td>23.570</td>\n","      <td>25.53</td>\n","      <td>152.50</td>\n","      <td>1709.0</td>\n","      <td>0.14440</td>\n","      <td>0.42450</td>\n","      <td>0.4504</td>\n","      <td>0.2430</td>\n","      <td>0.3613</td>\n","      <td>0.08758</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>84348301</td>\n","      <td>M</td>\n","      <td>11.42</td>\n","      <td>20.38</td>\n","      <td>77.58</td>\n","      <td>386.1</td>\n","      <td>0.14250</td>\n","      <td>0.28390</td>\n","      <td>0.24140</td>\n","      <td>0.10520</td>\n","      <td>0.2597</td>\n","      <td>0.09744</td>\n","      <td>0.4956</td>\n","      <td>1.1560</td>\n","      <td>3.445</td>\n","      <td>27.23</td>\n","      <td>0.009110</td>\n","      <td>0.07458</td>\n","      <td>0.05661</td>\n","      <td>0.01867</td>\n","      <td>0.05963</td>\n","      <td>0.009208</td>\n","      <td>14.910</td>\n","      <td>26.50</td>\n","      <td>98.87</td>\n","      <td>567.7</td>\n","      <td>0.20980</td>\n","      <td>0.86630</td>\n","      <td>0.6869</td>\n","      <td>0.2575</td>\n","      <td>0.6638</td>\n","      <td>0.17300</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>84358402</td>\n","      <td>M</td>\n","      <td>20.29</td>\n","      <td>14.34</td>\n","      <td>135.10</td>\n","      <td>1297.0</td>\n","      <td>0.10030</td>\n","      <td>0.13280</td>\n","      <td>0.19800</td>\n","      <td>0.10430</td>\n","      <td>0.1809</td>\n","      <td>0.05883</td>\n","      <td>0.7572</td>\n","      <td>0.7813</td>\n","      <td>5.438</td>\n","      <td>94.44</td>\n","      <td>0.011490</td>\n","      <td>0.02461</td>\n","      <td>0.05688</td>\n","      <td>0.01885</td>\n","      <td>0.01756</td>\n","      <td>0.005115</td>\n","      <td>22.540</td>\n","      <td>16.67</td>\n","      <td>152.20</td>\n","      <td>1575.0</td>\n","      <td>0.13740</td>\n","      <td>0.20500</td>\n","      <td>0.4000</td>\n","      <td>0.1625</td>\n","      <td>0.2364</td>\n","      <td>0.07678</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>564</th>\n","      <td>926424</td>\n","      <td>M</td>\n","      <td>21.56</td>\n","      <td>22.39</td>\n","      <td>142.00</td>\n","      <td>1479.0</td>\n","      <td>0.11100</td>\n","      <td>0.11590</td>\n","      <td>0.24390</td>\n","      <td>0.13890</td>\n","      <td>0.1726</td>\n","      <td>0.05623</td>\n","      <td>1.1760</td>\n","      <td>1.2560</td>\n","      <td>7.673</td>\n","      <td>158.70</td>\n","      <td>0.010300</td>\n","      <td>0.02891</td>\n","      <td>0.05198</td>\n","      <td>0.02454</td>\n","      <td>0.01114</td>\n","      <td>0.004239</td>\n","      <td>25.450</td>\n","      <td>26.40</td>\n","      <td>166.10</td>\n","      <td>2027.0</td>\n","      <td>0.14100</td>\n","      <td>0.21130</td>\n","      <td>0.4107</td>\n","      <td>0.2216</td>\n","      <td>0.2060</td>\n","      <td>0.07115</td>\n","    </tr>\n","    <tr>\n","      <th>565</th>\n","      <td>926682</td>\n","      <td>M</td>\n","      <td>20.13</td>\n","      <td>28.25</td>\n","      <td>131.20</td>\n","      <td>1261.0</td>\n","      <td>0.09780</td>\n","      <td>0.10340</td>\n","      <td>0.14400</td>\n","      <td>0.09791</td>\n","      <td>0.1752</td>\n","      <td>0.05533</td>\n","      <td>0.7655</td>\n","      <td>2.4630</td>\n","      <td>5.203</td>\n","      <td>99.04</td>\n","      <td>0.005769</td>\n","      <td>0.02423</td>\n","      <td>0.03950</td>\n","      <td>0.01678</td>\n","      <td>0.01898</td>\n","      <td>0.002498</td>\n","      <td>23.690</td>\n","      <td>38.25</td>\n","      <td>155.00</td>\n","      <td>1731.0</td>\n","      <td>0.11660</td>\n","      <td>0.19220</td>\n","      <td>0.3215</td>\n","      <td>0.1628</td>\n","      <td>0.2572</td>\n","      <td>0.06637</td>\n","    </tr>\n","    <tr>\n","      <th>566</th>\n","      <td>926954</td>\n","      <td>M</td>\n","      <td>16.60</td>\n","      <td>28.08</td>\n","      <td>108.30</td>\n","      <td>858.1</td>\n","      <td>0.08455</td>\n","      <td>0.10230</td>\n","      <td>0.09251</td>\n","      <td>0.05302</td>\n","      <td>0.1590</td>\n","      <td>0.05648</td>\n","      <td>0.4564</td>\n","      <td>1.0750</td>\n","      <td>3.425</td>\n","      <td>48.55</td>\n","      <td>0.005903</td>\n","      <td>0.03731</td>\n","      <td>0.04730</td>\n","      <td>0.01557</td>\n","      <td>0.01318</td>\n","      <td>0.003892</td>\n","      <td>18.980</td>\n","      <td>34.12</td>\n","      <td>126.70</td>\n","      <td>1124.0</td>\n","      <td>0.11390</td>\n","      <td>0.30940</td>\n","      <td>0.3403</td>\n","      <td>0.1418</td>\n","      <td>0.2218</td>\n","      <td>0.07820</td>\n","    </tr>\n","    <tr>\n","      <th>567</th>\n","      <td>927241</td>\n","      <td>M</td>\n","      <td>20.60</td>\n","      <td>29.33</td>\n","      <td>140.10</td>\n","      <td>1265.0</td>\n","      <td>0.11780</td>\n","      <td>0.27700</td>\n","      <td>0.35140</td>\n","      <td>0.15200</td>\n","      <td>0.2397</td>\n","      <td>0.07016</td>\n","      <td>0.7260</td>\n","      <td>1.5950</td>\n","      <td>5.772</td>\n","      <td>86.22</td>\n","      <td>0.006522</td>\n","      <td>0.06158</td>\n","      <td>0.07117</td>\n","      <td>0.01664</td>\n","      <td>0.02324</td>\n","      <td>0.006185</td>\n","      <td>25.740</td>\n","      <td>39.42</td>\n","      <td>184.60</td>\n","      <td>1821.0</td>\n","      <td>0.16500</td>\n","      <td>0.86810</td>\n","      <td>0.9387</td>\n","      <td>0.2650</td>\n","      <td>0.4087</td>\n","      <td>0.12400</td>\n","    </tr>\n","    <tr>\n","      <th>568</th>\n","      <td>92751</td>\n","      <td>B</td>\n","      <td>7.76</td>\n","      <td>24.54</td>\n","      <td>47.92</td>\n","      <td>181.0</td>\n","      <td>0.05263</td>\n","      <td>0.04362</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.1587</td>\n","      <td>0.05884</td>\n","      <td>0.3857</td>\n","      <td>1.4280</td>\n","      <td>2.548</td>\n","      <td>19.15</td>\n","      <td>0.007189</td>\n","      <td>0.00466</td>\n","      <td>0.00000</td>\n","      <td>0.00000</td>\n","      <td>0.02676</td>\n","      <td>0.002783</td>\n","      <td>9.456</td>\n","      <td>30.37</td>\n","      <td>59.16</td>\n","      <td>268.6</td>\n","      <td>0.08996</td>\n","      <td>0.06444</td>\n","      <td>0.0000</td>\n","      <td>0.0000</td>\n","      <td>0.2871</td>\n","      <td>0.07039</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>569 rows × 32 columns</p>\n","</div>"],"text/plain":["           0  1      2      3   ...      28      29      30       31\n","0      842302  M  17.99  10.38  ...  0.7119  0.2654  0.4601  0.11890\n","1      842517  M  20.57  17.77  ...  0.2416  0.1860  0.2750  0.08902\n","2    84300903  M  19.69  21.25  ...  0.4504  0.2430  0.3613  0.08758\n","3    84348301  M  11.42  20.38  ...  0.6869  0.2575  0.6638  0.17300\n","4    84358402  M  20.29  14.34  ...  0.4000  0.1625  0.2364  0.07678\n","..        ... ..    ...    ...  ...     ...     ...     ...      ...\n","564    926424  M  21.56  22.39  ...  0.4107  0.2216  0.2060  0.07115\n","565    926682  M  20.13  28.25  ...  0.3215  0.1628  0.2572  0.06637\n","566    926954  M  16.60  28.08  ...  0.3403  0.1418  0.2218  0.07820\n","567    927241  M  20.60  29.33  ...  0.9387  0.2650  0.4087  0.12400\n","568     92751  B   7.76  24.54  ...  0.0000  0.0000  0.2871  0.07039\n","\n","[569 rows x 32 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"cCogdhBORt9g","colab_type":"code","colab":{}},"source":["x = np.zeros((569, 1), dtype=float) #set array to store the serial number\n","y = np.zeros((569, 30), dtype=float) #set array to store the values\n","z = np.zeros((569,), dtype=str)# #set array to store the labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAPWwM8lRt9l","colab_type":"code","colab":{}},"source":["#read the dataset into array\n","with open(data_path, 'r') as input_file:\n","    reader = csv.reader(input_file)\n","    for i, row in enumerate(reader):\n","        data_name = [float(row[0])] \n","        x[i] = data_name\n","        data_value = [float(datum) for datum in row[2:] ]\n","        y[i] = data_value  \n","        z[i] = row[1] #label\n","#     y = np.concatenate((x, y), axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TKFA0J51Rt9q","colab_type":"code","colab":{}},"source":["#Encode the labels with value 1 and  0\n","le = preprocessing.LabelEncoder()\n","le.fit([\"M\", \"B\"])  #Label: M(M=malignant) is 1, B(B=benign) is 0.\n","z = le.transform(z)\n","# print('label standard:%s' % data_label)\n","# print('reverse the label:%s' % le.inverse_transform([0, 0, 1]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"vrBQCGwsRt9s","colab_type":"code","colab":{}},"source":["#Split the dataset into training set and test set\n","y_train, y_test, z_train, z_test = train_test_split(y, z, test_size = 0.2, random_state = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSmH6idmSz50","colab_type":"text"},"source":["#Transformer and Estimator step by step"]},{"cell_type":"code","metadata":{"id":"1Plxo4xRRt9v","colab_type":"code","colab":{}},"source":["#============================================================="],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j-_m8ajjUlh2","colab_type":"text"},"source":["Feature scaling"]},{"cell_type":"code","metadata":{"id":"t7BvvS8VRt9y","colab_type":"code","colab":{}},"source":["ytrain_transformed = StandardScaler().fit_transform(y_train)\n","ytest_transformed = StandardScaler().fit_transform(y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mvZpllreUp6G","colab_type":"text"},"source":["Use PCA to reduce the dimensionality"]},{"cell_type":"code","metadata":{"id":"vugc1Ka-Rt91","colab_type":"code","colab":{}},"source":["pca = PCA(n_components=2)\n","y_train_pca = pca.fit_transform(ytrain_transformed)\n","y_test_pca = pca.fit_transform(ytest_transformed)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ou8cZuSaUvMR","colab_type":"text"},"source":["Classifier"]},{"cell_type":"code","metadata":{"id":"7NBRkYPMRt93","colab_type":"code","outputId":"7076a44b-9fef-4622-9870-d4ad54c41c53","executionInfo":{"status":"ok","timestamp":1589146875208,"user_tz":-120,"elapsed":616,"user":{"displayName":"Gerrit Merz","photoUrl":"","userId":"17174744278973723577"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["classifier = LogisticRegression(random_state=1, solver='liblinear')\n","classifier.fit(y_train_pca, z_train) #z_train is label\n","print('Test accuracy: %.3f' % classifier.score(y_test_pca, z_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test accuracy: 0.789\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SjglhG5iRt97","colab_type":"code","colab":{}},"source":["#============================================================="],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Ut90YhVSos7","colab_type":"text"},"source":["#Use pipeline to pack all steps(Transformer, Estimator)\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"Fxjk04UdRt99","colab_type":"code","outputId":"710151ab-50e4-4c26-b494-3bcff0a01bb5","executionInfo":{"status":"ok","timestamp":1589146877557,"user_tz":-120,"elapsed":832,"user":{"displayName":"Gerrit Merz","photoUrl":"","userId":"17174744278973723577"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pipeline = Pipeline([\n","    ('sc', StandardScaler()),\n","    ('pca', PCA(n_components=2)),\n","    ('clf', LogisticRegression(random_state=1, solver='liblinear'))\n","])\n","pipeline.fit(y_train, z_train)\n","print('Test accuracy: %.3f' % pipeline.score(y_test, z_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test accuracy: 0.947\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PXnWAv4TT2oN","colab_type":"text"},"source":["# Recursive Feature Elimination"]},{"cell_type":"code","metadata":{"id":"ZBgThfWVT1x4","colab_type":"code","outputId":"2979c001-2c2e-4920-b9b7-076b674842ec","executionInfo":{"status":"ok","timestamp":1589146883710,"user_tz":-120,"elapsed":2982,"user":{"displayName":"Gerrit Merz","photoUrl":"","userId":"17174744278973723577"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["selector = RFECV(LogisticRegression(random_state=1, solver='liblinear'), step=1, cv=5)\n","selector = selector.fit(y, z)\n","print(selector.support_)\n","print(selector.ranking_)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[ True  True  True False  True  True  True  True  True  True  True  True\n","  True  True False False  True  True  True False  True  True  True  True\n","  True  True  True  True  True  True]\n","[1 1 1 5 1 1 1 1 1 1 1 1 1 1 2 3 1 1 1 4 1 1 1 1 1 1 1 1 1 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"frj2mZYmfuNt","colab_type":"text"},"source":["We see that 26 of the 30 features are relevant. The following features (according to their index number) are relevant and hence used."]},{"cell_type":"code","metadata":{"id":"utXbYs4AfObC","colab_type":"code","colab":{}},"source":["y_train_rfe = y_train[:,[0,1,2,4,5,6,7,8,9,10,11,12,13,16,17,18,20,21,22,23,24,24,26,27,28,29]]\n","y_test_rfe = y_test[:,[0,1,2,4,5,6,7,8,9,10,11,12,13,16,17,18,20,21,22,23,24,24,26,27,28,29]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqGENoOxfX7N","colab_type":"code","outputId":"37af8a56-aa0f-4f6c-d34f-e5de896e2658","executionInfo":{"status":"ok","timestamp":1589146935001,"user_tz":-120,"elapsed":741,"user":{"displayName":"Gerrit Merz","photoUrl":"","userId":"17174744278973723577"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pipeline = Pipeline([\n","    ('sc', StandardScaler()),\n","    ('clf', LogisticRegression(random_state=1, solver='liblinear'))\n","])\n","pipeline.fit(y_train_rfe, z_train)\n","print('Test accuracy: %.3f' % pipeline.score(y_test_rfe, z_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test accuracy: 0.974\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cq5Ihyk6gCH9","colab_type":"text"},"source":["With the Feature Elimination instead of the PCA we improve our accuracy score to 97.4%"]},{"cell_type":"markdown","metadata":{"id":"0E2YVHWpU3TA","colab_type":"text"},"source":["#Conclusion"]},{"cell_type":"markdown","metadata":{"id":"jMqUYc_VTSux","colab_type":"text"},"source":["In a typical machine learning workflow you will need to apply all these transformations at least twice. Once when training the model and again on any new data you want to predict on. Of course you could write a function to apply them and reuse that but you would still need to run this first and then call the model separately. Scikit-learn pipelines are a tool to simplify this process. They have several key benefits:\n","*   They make your workflow much easier to read and understand.\n","*   They enforce the implementation and order of steps in your project.\n","*   These in turn make your work much more reproducible.\n","\n","Scikit-learn has built in functions for most of these commonly used transformations in it’s package.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"0mBHwacGRt9_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}